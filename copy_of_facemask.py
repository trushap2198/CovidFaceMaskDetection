# -*- coding: utf-8 -*-
"""Copy of FaceMask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k0ptU3ncUjkRReI24PvdAQQpfWAgPpCd
"""

from torch.nn import *
from torch.utils.data import DataLoader, Dataset
import torch
import numpy as np
from torchvision import transforms 
from pathlib import Path
#import pandas as pd
from PIL import Image
import os
import sklearn
import pickle
from pathlib import Path
from sklearn.model_selection import train_test_split
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

from google.colab import drive
drive.mount('/content/drive')

#configurations
translate_label = {'No_Mask': 0, 'Cloth_Mask': 1, 'N95_Mask': 2, 'N95_With_Valve': 3, 'Surgical_Mask': 4}
HISTORY_PATH = '/content/trainHistoryDict'
SPLIT_DATA_FILE = "splitData.pkl"
image_type = "RGB"
root = '/content/drive/MyDrive'
models_path= "/content/models/"
pre_data_path = "/content/drive/MyDrive/mask_dataset/"
org_data_path = "/content/drive/MyDrive/mask_dataset"
#org_data_path = "/content/drive/MyDrive/preprocess"
img_size = 125
category_map = ['No_Mask', 'Cloth_Mask', 'N95_Mask', 'N95_With_Valve', 'Surgical_Mask']

def get_image_list(path, category):
    return list(sorted(os.listdir(os.path.join(path, category))))


def get_images_path(imgs, path):
    return [(i, path + "/" + i) for i in imgs]


def transform_img(img_path, size):
    return Image.open(img_path).convert("RGB").resize((size, size), Image.BILINEAR)


def load_dataset(data_set_path, category_map):
    maskDict = {}
    for i in category_map:
        maskDict[i] = []
    for category in category_map:
        sub_dirc = data_set_path / category;
        for img in list(sub_dirc.iterdir()):
            maskDict[category].append(img)
    return maskDict


def get_cat(datasetPath):
    categories = {}
    for i, category_path in enumerate(list(datasetPath.iterdir())):
        if category_path.is_dir():
            category = category_path.name.split("/")[-1]
            categories[category] = i
    return categories


def convertImages(maskDict, org_data_path, pre_data):
    if (not os.path.exists(pre_data)):
        print("Creating preprocessing directory..")
        os.mkdir(pre_data)
    for item in maskDict:
        if (not os.path.exists(pre_data + "/" + item)):
            print("The directory file does not exist. Making one")
            os.mkdir(pre_data + "/" + item)

        img_lst = get_image_list(org_data_path, item)
        imgs = get_images_path(img_lst, org_data_path + "/" + item)
        for i in imgs:
            try:
                img = transform_img(i[1], img_size)
                new_path = pre_data + "/" + item + "/" + i[0]
                img.save(new_path)
            except:
                continue

datasetPath = Path(org_data_path)
category_map = get_cat(datasetPath)
data_dict = load_dataset(datasetPath, category_map)
convertImages(data_dict, org_data_path, Path(pre_data_path).name)

category_map

import pandas as pd
csv_path  =  "/content/drive/MyDrive/dataset.xlsx"
def load_from_csv(csv_path):
    df = pd.read_excel (csv_path)
    print (df)
    data = df.values.tolist()
    return data

data = load_from_csv(csv_path)

from torch.cuda.memory import list_gpu_processes
for list_v in  data:
    temp = list_v[1]
    list_v[1] = pre_data_path + list_v[2] +"/" +temp
    #print(list_v)

# def load_dataset(data_set_path):
#     data = []
#     for category in category_map:
#         sub_dir = data_set_path / category
#         if sub_dir.exists():
#             for img in list(sub_dir.iterdir()):
#                 data.append((img, category))
#         else:
#             print("No folder of category", category)
#     return data
# data = load_dataset(Path(pre_data_path))

#Number of males
male_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
count = 0
for row in data:
    if row[4] == 'M':
        male_count[row[2]] = male_count[row[2]] + 1
        count = count + 1
print(count)

#Number of femalesp
female_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
count = 0
for row in data:
    if row[4] == 'F':
        female_count[row[2]] =female_count[row[2]] + 1
        count+= 1
print(count)

#Number of mid
mid_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
count = 0
for row in data:
    if row[3] == 'mid':
        mid_count[row[2]] = mid_count[row[2]] + 1
        count+=1
print(count)

#Number of mid
young_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
count = 0
for row in data:
    if row[3] == 'young':
        young_count[row[2]] = young_count[row[2]] + 1
        count+=1
print(count)

#Number of mid
old_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
count = 0
for row in data:
    if row[3] == 'old':
        old_count[row[2]] = old_count[row[2]] + 1
        count+=1
print(count)

l = len(data)
print(l)
test_len = int(0.8 * l)
#Stratified split
train_set, test_set = train_test_split(data, test_size= 0.2, shuffle=True )
print(len(train_set))
print(len(test_set))
import pickle
pickle.dump(test_set, open("test.pkl", "wb"))
pickle.dump(train_set, open("train.pkl", "wb"))

# train_set_No_Mask = [ for _, i in train_set[1] if i == 'No_Mask']
# train_set_No_Mask = [ for _, i in train_set[1] if i == 'No_Mask']
# train_set_No_Mask = [ for _, i in train_set[1] if i == 'No_Mask']
# train_set_No_Mask = [ for _, i in train_set[1] if i == 'No_Mask']
# train_set_No_Mask = [ for _, i in train_set[1] if i == 'No_Mask']
def oversample(dataset):
  dic = {'No_Mask' : [], 'Cloth_Mask' : [], 'N95_Mask' : [], 'N95_With_Valve' : [], 'Surgical_Mask':[]}
  for data in dataset:
    dic[data[2]].append(data)
  l = list(dic.keys())
  maxkey = max(l, key = lambda x : len(dic[x]))
  max_size = len(dic[maxkey])
  data_oversample = []
  for i in dic:
    if i is not maxkey:
      print(i)
      dic[i] = sklearn.utils.resample(dic[i], 
                                 replace=True,    # sample with replacement
                                 n_samples= max_size)
    data_oversample = data_oversample + dic[i]
  print([len(dic[i]) for i in dic ])
  return sklearn.utils.shuffle(data_oversample)
oversample(train_set)
print("")

class ImageDataset(Dataset):
    def __init__(self, data, transform_img=None, transform_label=None):
        self.transform_img = transforms.Compose(
            [transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406),
                                       (0.229, 0.224, 0.225))
            ])
        self.transform_label = transform_label
        # To Fix for images
        self.training_data_X = [i[1] for i in data]
        self.training_data_y = [i[2] for i in data]

    def __len__(self):
        return len(self.training_data_y)

    def __getitem__(self, idx):
        image = Image.open(self.training_data_X[idx]).convert(image_type)
        if self.transform_img:
            image = self.transform_img(image)
        label = self.training_data_y[idx]
        if self.transform_label:
            label = torch.tensor(self.transform_label[label])
        return image, label

ImageDataset(train_set)[4]

def get_data_loader(train_set, transform_label=translate_label, batch_size=30):
    return DataLoader(ImageDataset(train_set, transform_label=translate_label),
                      batch_size=batch_size, shuffle=True)    
# train_dataloader = get_data_loader(train_set)
# validation_dataloader = get_data_loader(val_set)

from torch.nn import *



class FaceDetectionModel(Module):
    def __init__(self, inchannels=3, kernal=5, img_size=125, output_classes=5):
        super(FaceDetectionModel, self).__init__()
        """The initialization method is used to initialize all the components needed
        to implement our neural model. In this case, we have to initialize a 
        recurrent neural network and a linear transformation."""

        # Initialize the RNN (using torch.nn.RNN). Use batch_first=True as input argument.
        # Your code here. Aim for 1-2 line.
        self.output_of_cnn = 300
        # For calculations
        # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool2d#torch.nn.MaxPool2d
        self.cnn = Sequential(
            #BatchNorm2d(inchannels),
            # Input size (N,inchannels, 225,225)
            Conv2d(inchannels, 64, kernal, stride=1, padding='same'),
            BatchNorm2d(64),
            LeakyReLU(),
            MaxPool2d(3),
            # Output size (N,10, 45,45)

            # Input size (N,10, 45,45)
            Conv2d(64, 64, kernal, stride=1, padding='same'),
            BatchNorm2d(64),
            LeakyReLU(),
            MaxPool2d(3, stride=2),

            Conv2d(64, 128, kernal, stride=1, padding='same'),
            BatchNorm2d(128),
            LeakyReLU(),
            MaxPool2d(3, stride=2),
            # # Input size (N,10, 45,45)
            # Conv2d(20, 30, kernal, stride=1, padding='same'),
            # LeakyReLU(),
            # LayerNorm(((((img_size - kernal + 1) // 5) + 1 - kernal + 1) // 2) + 1),
            # MaxPool2d(kernal, stride=1, padding=kernal // 2),

            # Input size (N,10, 9,9)
            Conv2d(128, 64, kernal, stride=1, padding='same'),
            BatchNorm2d(64),
            LeakyReLU(),
            MaxPool2d(3, stride=3),
            # Input size (N,8, 9,9)

            torch.nn.Flatten(),
            Linear(576,
                   self.output_of_cnn)
        )

        # Initialize the Linear transformation (using torch.nn.Linear)
        # Your code here. Aim for 1 lines.
        self.dnn = Sequential(
            Linear(self.output_of_cnn, 400),
            Dropout(0.8),
            LeakyReLU(),
            LayerNorm(400),

            Linear(400, 500),
            Dropout(0.8),
            LeakyReLU(),
            LayerNorm(500),

            Linear(500, 125),
            Dropout(),
            LeakyReLU(),
            LayerNorm(125),
            Linear(125, output_classes))

    def forward(self, X):
        Z = self.cnn(X)
        out = self.dnn(Z)
        #         if not self.training:
        #           # Your code here. Aim for 1-2 lines.
        #           # Remember to apply a threshold on top of the sigmoid to get the prediction.
        #           out = torch.softmax(out.detach(), dim=1)
        #           out = out.argmax(1)
        return out

if __name__ == "__main__":
    model = FaceDetectionModel(img_size=img_size, output_classes=5)
    for param in model.parameters():
        print(param.shape)

# def load_dataset(data_set_path):
#     data = []
#     for category in category_map:
#         sub_dir = data_set_path / category
#         if sub_dir.exists():
#             for img in list(sub_dir.iterdir()):
#                 data.append((img, category))
#         else:
#             print("No folder of category", category)
#     return data

def train(dataloader, model, loss_fn, optimizer, print_info=lambda batch: batch % 100 == 0):
    predictions, real = torch.tensor([]), torch.tensor([])
    predictions, real = predictions.to(device), real.to(device)
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.train()
    loss_val = 0
    total_loss, correct = 0, 0
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        output = model(X)

        loss = loss_fn(output, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss = total_loss + loss.item()
        pred = torch.argmax(output, dim=1)
        correct = correct + (pred == y).sum().item()
        predictions = torch.cat((predictions, pred))
        real = torch.cat((real, y))
        loss_val, current = loss.item(), batch * len(X)
        if print_info(batch):
            print(f"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]")
    total_loss /= num_batches
    correct /= size
    print(f"Training Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {total_loss:>8f} \n")
    return total_loss, correct, real, predictions

def validation(dataloader, model, loss_fn):

    predictons, real = torch.tensor([]), torch.tensor([])
    predictons, real = predictons.to(device), real.to(device)
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            output = model(X)
            test_loss = test_loss + loss_fn(output, y).item()
            pred = torch.argmax(output, dim=1)
            correct = correct + (pred == y).sum().item()
            predictons = torch.cat((predictons, pred))
            real = torch.cat((real, y))
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    return test_loss, correct, real, predictons

##KFOLD Train
from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit
#kfold = StratifiedShuffleSplit(n_splits=10)
kfold = StratifiedShuffleSplit(n_splits=6)
#data = load_dataset(Path(pre_data_path))
historyK = []
models = []
# train_set, val_set = train_test_split(data, test_size=0.2, shuffle=True)
for fold, (train_ids, test_ids) in enumerate(kfold.split(train_set, [ translate_label[i[2]] for i in train_set])):
    # pre_data_path = sys.argv[1]
    torch.manual_seed(0)
    np.random.seed(0)
    model = FaceDetectionModel(img_size=img_size, output_classes=5)
    model.cuda(device)
    print("train data ", len(train_ids))
    print("test data ", len(test_ids))
    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
    # if len(sys.argv) > 4:
    #     print("Loading Model")
    #     model_name = sys.argv[2]
    #     model.load_state_dict(torch.load(model_name))
    #     print("Loading Split data")
    #     data = pickle.load(open(sys.argv[3], "rb"))
    #     train_set = data['train']
    #     val_set = data['test']
    #     history = pickle.load(open(sys.argv[4], "rb"))
    # else:
    print("fold ", fold)

    # with open(SPLIT_DATA_FILE, 'wb') as file_pi:
    #     pickle.dump({'train': train_set, 'test': val_set}, file_pi)
    # torch.save(model.state_dict(), models_path + "/model0.pth")
    print("model shape:")
    loss_fn = CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 0.006)
    train_dataloader = get_data_loader([train_set[ids] for ids in train_ids], translate_label)
    validation_dataloader = get_data_loader([train_set[ids] for ids in test_ids], translate_label)
    start = len(history['train_loss'])
    for param in model.parameters():
        print(param.shape)
    for t in range(start, start + 40):
        print(f"Epoch {t + 1}\n-------------------------------")
        train_loss, train_acc, _, _ = train(train_dataloader, model, loss_fn, optimizer, print_info=lambda batch: batch % 10 == 0)
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        val_loss, val_acc, real, predication = validation(validation_dataloader, model, loss_fn)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['real'].append(real)
        history['predication'].append(predication)
        #torch.save(model.state_dict(), models_path + "/model" + str(t + 1) + ".pth")
    print("Done!")
    historyK.append(history)
    models.append(model)

with open("data.pkl", 'wb') as file_pi:
    pickle.dump(historyK, file_pi)

with open ("model6.pkl", "wb") as file_pi:
    pickle.dump(models[5],file_pi)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
def show_confusion_matrix(y_true, y_preds, name):
    matrix = confusion_matrix(y_true.cpu().numpy(), y_preds.cpu().numpy())
    print("Showing confusion matrix for " + name)
    print(matrix)
    plt.figure(figsize=(15, 10))
    ax = sns.heatmap(matrix, fmt='', annot=True, cmap='Reds')
    ax.set_title('Confusion Matrix')
    ax.set_xlabel('Predicted Mask Type')
    ax.set_ylabel('Actual Mask Type')
    ax.xaxis.set_ticklabels([i for i in translate_label.keys()])
    ax.yaxis.set_ticklabels([i for i in translate_label.keys()])
    print(name)
    #plt.savefig(name)
    plt.show()
    plt.close()

def plot_loss(train_loss, val_loss):
    """ Plot the losses in each epoch"""
    plt.plot(train_loss, '-bx')
    plt.plot(val_loss, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    plt.title('Loss vs. No. of epochs')
    #plt.savefig('plots/plot_loss.png')
    plt.show()
    plt.close()

def plot_accuracies(train_accuracy,val_accuracy):
    """ Plot the history of accuracies"""
    plt.plot(train_accuracy, '-bx')
    plt.plot(val_accuracy, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.legend(['Training', 'Validation'])
    plt.ylim(0,1)
    plt.title('Accuracy vs. No. of epochs');
    #plt.savefig('plots/plot_accuracy.png')
    plt.show()
    plt.close()

with open("data.pkl", 'wb') as file_pi:
    pickle.dump(historyK, file_pi)

torch.save(model.state_dict(), 'model_kfold.pkl')

# import matplotlib.pyplot as plt

# for history in historyK:
#   plot_loss(history['train_loss'], history['val_loss'])
# for history in historyK:
#   plot_accuracies(history['train_acc'], history['val_acc'])

history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(test_set, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[6], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)

show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))

female = []
for row in test_set:
    if row[4] == 'F':
        female.append(row)
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(female, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[3], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)
show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))

#Male dataset:
#female_count = {'Cloth_Mask': 0, "No_Mask":0,"N95_Mask" : 0, "N95_With_Valve": 0, "Surgical_Mask": 0}
male = []
for row in test_set:
    if row[4] == 'M':
        male.append(row)
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(male, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[3], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)
show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))

mid = []
for row in test_set:
    if row[3] == 'mid':
        mid.append(row)
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(mid, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[3], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)
show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))

from matplotlib import test
young = []
for row in test_set:
    if row[3] == 'young':
        young.append(row)
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(young, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[3], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)
show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))

from matplotlib import test
old = []
for row in test_set:
    if row[3] == 'old':
        old.append(row)
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': [], 'real': [], 'predication': []}
validation_dataloader = get_data_loader(old, translate_label)
val_loss, val_acc, real, predication = validation(validation_dataloader, models[3], loss_fn)
history['val_loss'].append(val_loss)
history['val_acc'].append(val_acc)
history['real'].append(real)
history['predication'].append(predication)
show_confusion_matrix(history['real'][0], history['predication'][0], "asd")
print(classification_report(history['real'][0].cpu(), history['predication'][0].cpu()))